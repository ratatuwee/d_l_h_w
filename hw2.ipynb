{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ad59b9-bc6c-417c-95d7-7f894c5883cd",
   "metadata": {},
   "source": [
    "# 2021120044_김형겸_hw2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd61ec-6824-46e4-8f99-8c86834e4d13",
   "metadata": {},
   "source": [
    "## 요구사항 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2098bea2264586",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84842d5-48d5-4241-9eba-4b4095164dc3",
   "metadata": {},
   "source": [
    "### init\n",
    "X = 입력 특성을 나타내는 데이터, Float\n",
    "y = 타깃 레이블, Long\n",
    "\n",
    "### len\n",
    "데이터셋의 샘플 수 반환, X의 길이 반환\n",
    "\n",
    "### getitem\n",
    "idx의 해당하는 입력 특성과 타깃을 딕셔너리 형태로 반환\n",
    "\n",
    "### str\n",
    "데이터셋 정보 표현\n",
    "데이터셋의 샘플 수, 입력 특성 데이터 shape, 타깃 레이블 shape\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd161ab-f4c7-4dd9-a156-7244c5e2aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "      len(self.X), self.X.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7e7a4-e786-4243-8a8c-83adc3ab067c",
   "metadata": {},
   "source": [
    "### init\n",
    "생성자에서 데이터의 특성 X를 입력 받아 FloatTensor를 이용하여 텐서화\n",
    "\n",
    "### len\n",
    "데이터셋의 샘플 수 반환, X의 길이 반환\n",
    "\n",
    "### getitem\n",
    "idx의 해당하는 입력 특성 샘플 반환\n",
    "\n",
    "### str\n",
    "데이터셋 정보 표현\n",
    "데이터셋의 샘플 수, 입력 특성 데이터 shape\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d59f71-2816-4125-b6f6-67283481c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    #1\n",
    "    CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    #2\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    #3\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    #print(dataset)\n",
    "\n",
    "    #4\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "    #print(test_dataset)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acb826-b92e-4b36-ba48-539e5a494a27",
   "metadata": {},
   "source": [
    "### 1\n",
    "파일 경로를 설정하고 훈련, 테스트 데이터 경로 설정하여 가지고 옴\n",
    "\n",
    "### 2\n",
    "데이터의 전처리 과정 get_preprocessed_daraset_1~6까지의 함수를 이용\n",
    "\n",
    "### 3\n",
    "훈련, 테스트 데이터 분할\n",
    "\n",
    "\n",
    "훈련 데이터\n",
    ">데이터의 타겟 변수와 아닌 부분으로 나누어 데이터 저장\n",
    "\n",
    "\n",
    "테스트 데이터\n",
    ">타겟 변수 데이터 제외하고 데이터 저장\n",
    "\n",
    "### 4\n",
    "3에서 만든 훈련 데이터를 random_split을 사용하여 8:2로 분리하여 train_dataset, validation_dataset로 나누고 이렇게 만들어진 train_dataset, validation_dataset, test_dataset를 반환\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d71359-975b-41f1-9146-7ac5eb155c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b08209-8a5c-4e68-8e5f-ee12fcfb8fd1",
   "metadata": {},
   "source": [
    "### 데이터 전처리 과정 1\n",
    "Pclass(티켓의 클래스), Fare(요금)을 이용하여 Pclass 별로 그룹화 하여 각 Pclass Fare의 평균값을 계산하고 Fare_mean 데이터프레임으로 저장\n",
    "\n",
    "\n",
    "Fare 결측치(NaN)를 확인하고 격측치에 평균 값을 넣어서 결측지가 없게 데이터 처리\n",
    "\n",
    "\n",
    "__결측치(데이터의 값이 없는 것, NaN)__\n",
    "\n",
    "\n",
    "DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=None, indicator=False, validate=None)\n",
    ">데이터베이스 스타일 조인을 사용하여 병합\n",
    ">\n",
    ">\n",
    ">>__주의 : 두 키 열 모두에 키가 Null 값인 행이 포합되어 있으면 일치되어 예기치 않는 결과 발생 가능__\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f65958-3004-4d9f-9612-9174885422b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8a64d-c956-4676-a0a1-7b0501493ce3",
   "metadata": {},
   "source": [
    "### 데이터 전처리 2\n",
    "이름 데이터의 있는 3개의 컬럼 분리하여 저장\n",
    "\n",
    "\n",
    "성, 이름, 호칭으로 구성되어 있는 name을 각 컬럼으로 나누어 데이터 처리\n",
    "\n",
    "\n",
    "pandas.concat(objs, *, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=None)\n",
    ">특정 축을 이용하여 pasda 개체를 연결\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff8ca1-453b-4550-b485-1a8c463e643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803656a-0156-4653-93f9-13851803ec05",
   "metadata": {},
   "source": [
    "### 데이터 전처리 3\n",
    "데이터 전처리 1과 마찬가지로 나이의 결측치 부분 채우는 과정\n",
    "\n",
    "\n",
    "호칭별로 나이 중간값을 계산(반올림하여 정수로 표현)하고 결측치인 부분에 호칭에 맞게 계산된 중간값을 채워 데이터 처리\n",
    "\n",
    "\n",
    "DataFrame.drop(labels=None, *, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    ">행이나 열에서 지정된 레이블 삭제\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e5b54-5181-47c2-9192-9c0020509037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "\n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e75863-9a44-4fcc-b9c3-21787f437059",
   "metadata": {},
   "source": [
    "### 데이터 전처리 4\n",
    "데이터 열 추가(병합 등) 및 삭제\n",
    "\n",
    "\n",
    "family_num 추가\n",
    ">Parch(부모, 자녀 수) + SibSp(형제, 배우자 수)\n",
    "\n",
    "alone 추가\n",
    ">family_num == 0일 경우 1로 설정\n",
    "\n",
    "__fillna : 결측값 채우기, DataFrame.fillna(value=None, *, method=None, axis=None, inplace=False, limit=None, downcast=_NoDefault.no_default)__\n",
    ">__meteod 부분(ffill, bfill, interpolate, reindex, asfreq, None)__\n",
    ">\n",
    ">\n",
    ">>ffill : 마지막 유효한 관찰을 다음 유효한 관찰로 전파하여 값을 채움\n",
    ">>\n",
    ">>\n",
    ">>bfill : 공백을 메우기 위해 다음 유효한 관찰을 사용하여 값을 채움\n",
    ">>\n",
    ">>\n",
    ">>interpolate : 보간을 사용하여 채움\n",
    ">>\n",
    ">>\n",
    ">>reindex : 개체를 새 인덱스에 맞춤\n",
    ">>\n",
    ">>\n",
    ">>asfreq : TimeSeries를 지정된 빈도로 변환\n",
    "\n",
    "\n",
    "불필요 부분 제거 : \"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\" 학습에 정보 제공 필요없는 부분 제거\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f41c3c-828a-4435-a3df-372af145d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "    ),\n",
    "    \"honorific\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8f4cd-a9ed-4c1b-853c-b78feff8d75d",
   "metadata": {},
   "source": [
    "### 데이터 전처리 5\n",
    "값 축소 및 결측치 처리\n",
    "\n",
    "\n",
    "honorific 값 축소\n",
    ">\"Mr\", \"Miss\", \"Mrs\", \"Master\" 이외의 값 \"other\"로 대체\n",
    ">\n",
    ">\n",
    ">>카디널리티(고유한 값의 수) 줄여 데이터셋 간소화 및 불필요한 복잡성 줄임\n",
    "\n",
    "\n",
    "Embarked fillna함수 이용 결측치 missing로 채워 데이터 처리\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb6dfd-8c51-455f-90a2-db01a5580828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8d9c5-19d0-4c64-af7a-c50c18e2f745",
   "metadata": {},
   "source": [
    "### 데이터 전처리 6\n",
    "카테고리 문자열 값을 수치 값으로 변환\n",
    "\n",
    "\n",
    "LabelEncoder : 카테고리 변수의 고유한 값들을 수치형으로 매핑\n",
    ">ex)male, female => 0, 1\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae50de-0b3f-483b-89d7-25fd9ee6807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self, n_input, n_output):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, 30),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(30, 30),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(30, n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0b5df-1cf1-4edd-bca9-6199d588658a",
   "metadata": {},
   "source": [
    "### 모델 정의\n",
    "nn.Module, nn.Sequential 이용\n",
    ">1번 레이어 : 입력 피처 수에서 30개 피처로 매칭\n",
    ">\n",
    ">\n",
    ">2번 레이어 : 30개의 피처에서 30개의 피처로 매핑\n",
    ">\n",
    ">\n",
    ">3번 레이어 : 30개의 피처에서 출력 수로 매핑\n",
    "\n",
    "\n",
    "ReLU활성화 함수로 비선형성 도입\n",
    ">ReLU6, ELU, LeakuReLU, PReLU 등으로 사용 가능\n",
    "\n",
    "\n",
    "forword 함수\n",
    ">PyTorch 모델에서 반드시 구현해야하는 메소드, 입력 데이터의 순방향 전파 정의\n",
    ">\n",
    ">\n",
    ">모델의 출력 생성 후 반환\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a372a5-05ee-4de6-8e45-97db3ffa9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_loader):\n",
    "  print(\"[TEST]\")\n",
    "  batch = next(iter(test_data_loader))\n",
    "  print(\"{0}\".format(batch['input'].shape))\n",
    "  my_model = MyModel(n_input=11, n_output=2)\n",
    "  output_batch = my_model(batch['input'])\n",
    "  prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "  for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "      print(idx, prediction.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8acb6-8413-4d18-b267-9969c355e493",
   "metadata": {},
   "source": [
    "### 학습 모델 test\n",
    "아직 만들어 지지 않은 학습 모델을 이용하여 testdata를 돌려 결과값을 나타내는 함수\n",
    "\n",
    "\n",
    "input(특성) 11, output 2(PassengerId,Survived)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f354c-e3c1-4fa9-88bd-847fff5964f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "  print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "    len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "  ))\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(train_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'], sample['target']))\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  print(\"[TRAIN]\")\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "  print(\"[VALIDATION]\")\n",
    "  for idx, batch in enumerate(validation_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  test(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfdbd3-9bd9-42e1-9180-1e5d2d1ee759",
   "metadata": {},
   "source": [
    "### 실제 실행\n",
    "위에서 만든 함수를 이용하여 데이터 셋을 불러오고 해당 데이터 셋을 로드하여 학습 후 테스트를 진행하는 부분으로 아직 학습하는 부분은 구현되어 있지 않은 코드\n",
    "\n",
    "\n",
    "각각 출력값으로 데이터의 shape등을 확인할 수 있다\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a886a38-a7b5-4655-9d18-233bf335fa62",
   "metadata": {},
   "source": [
    "## 요구사항 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ece4c-cb16-4f6c-9922-f3f26ebec611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 추가 수정 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cc1fc-5dce-4ef0-8b3d-761a44051b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X):\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "      len(self.X), self.X.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset():\n",
    "    CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    #print(dataset)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "    #print(test_dataset)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass별 Fare 평균값을 사용하여 Fare 결측치 메우기\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "\n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "    ),\n",
    "    \"honorific\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self, n_input, n_output):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "      nn.PReLU(),\n",
    "      nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "      nn.PReLU(),\n",
    "      nn.Linear(40, 30),\n",
    "      nn.PReLU(),\n",
    "      nn.Linear(30, 20),\n",
    "      nn.PReLU(),\n",
    "      nn.Linear(20, 10),\n",
    "      nn.PReLU(),\n",
    "      nn.Linear(10, n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "  my_model = MyModel(n_input=11, n_output=2)\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "  return my_model, optimizer\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "  beforetrainingloss = 0.0\n",
    "  beforevalidationloss = 0.0\n",
    "\n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.CrossEntropyLoss()  # Use a built-in loss function\n",
    "  next_print_epoch = 100\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    for train_batch in train_data_loader:\n",
    "      output_train = model(train_batch['input'])\n",
    "      loss = loss_fn(output_train, train_batch['target'])\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in validation_data_loader:\n",
    "        output_validation = model(validation_batch['input'])\n",
    "        loss = loss_fn(output_validation, validation_batch['target'])\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": loss_train / num_trains,\n",
    "      \"Validation loss\": loss_validation / num_validations\n",
    "    })\n",
    "\n",
    "    if epoch >= next_print_epoch:\n",
    "      print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "        f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "      )\n",
    "      next_print_epoch += 100\n",
    "    # if beforetrainingloss-0.001 < (loss_train / num_trains)<beforetrainingloss+0.001 and beforevalidationloss-0.001 < (loss_validation / num_validations)<beforevalidationloss+0.001:\n",
    "    #   print(\n",
    "    #     f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "    #     f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "    #   )\n",
    "    #   break\n",
    "    # beforetrainingloss = loss_train / num_trains\n",
    "    # beforevalidationloss = loss_validation / num_validations\n",
    "\n",
    "\n",
    "def test(test_data_loader):\n",
    "  print(\"[TEST]\")\n",
    "  batch = next(iter(test_data_loader))\n",
    "  print(\"{0}\".format(batch['input'].shape))\n",
    "  my_model = MyModel(n_input=11, n_output=2)\n",
    "  output_batch = my_model(batch['input'])\n",
    "  prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "  list = []\n",
    "  for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "      print(idx, prediction.item())\n",
    "      list.append([idx, prediction.item()])\n",
    "  df = pd.DataFrame(list, columns=['PassengerId', 'Survived'])\n",
    "  df.to_csv('submission.csv', index=False, encoding='cp949')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=512, help=\"Batch size (int, default: 512)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=100, help=\"Number of training epochs (int, default:1_000)\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [40, 40],\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"my_model_training\",\n",
    "        entity=\"pkqwaszx\",\n",
    "        notes=\"My first wandb experiment\",\n",
    "        tags=[\"my_model\", \"titanic_dataset\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "    linear_model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    # print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "    #     len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "    # ))\n",
    "    #\n",
    "    # for idx, sample in enumerate(train_dataset):\n",
    "    #   print(\"{0} - {1}: {2}\".format(idx, sample['input'], sample['target']))\n",
    "\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # print(\"[TRAIN]\")\n",
    "    # for idx, batch in enumerate(train_data_loader):\n",
    "    #   print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "    #\n",
    "    # print(\"[VALIDATION]\")\n",
    "    # for idx, batch in enumerate(validation_data_loader):\n",
    "    #     print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "    wandb.watch(linear_model, log=\"all\")\n",
    "\n",
    "    training_loop(\n",
    "        model=linear_model,\n",
    "        optimizer=optimizer,\n",
    "        train_data_loader=train_data_loader,\n",
    "        validation_data_loader=validation_data_loader\n",
    "    )\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    test(test_data_loader)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee50dae-71ce-4dd5-8ea7-004e5ce58717",
   "metadata": {},
   "source": [
    "### 설명\n",
    "wandb 코드를 활용하고 더 나은 성능을 위해 Activation Function, 피처 수 변경하며 확인\n",
    "\n",
    "\n",
    "2023-10-13_15-50-02 : RELU 피처수 30\n",
    "\n",
    "\n",
    "2023-10-13_15-51-26 : RELU 피처수 20\n",
    "\n",
    "\n",
    "2023-10-13_15-52-38 : RELU 피처수 40\n",
    "\n",
    "\n",
    "2023-10-13_15-53-56 : RELU 피처수 50\n",
    "\n",
    "\n",
    "2023-10-13_16-02-04 : ELU 피처 수 50\n",
    "\n",
    "\n",
    "2023-10-13_16-04-44 : LeakyReLU 피처 수 50\n",
    "\n",
    "\n",
    "2023-10-13_16-06-05 : PReLU 피처 수 50\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664cb01-0e3b-4a09-8a00-56962637ce2a",
   "metadata": {},
   "source": [
    "### wandb 사이트\n",
    "https://wandb.ai/pkqwaszx/my_model_training?workspace=user-pkqwaszx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d17e35-f062-481e-89b1-db3fe9822b2d",
   "metadata": {},
   "source": [
    "## 요구사항 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6e2e3-aef1-405d-b6bc-ba6139f79d4f",
   "metadata": {},
   "source": [
    "코드 수정하며 에포크 길이 조절 및 레이어 수 변경(+피처 내림차) 및 csv 파일 생성\n",
    "\n",
    "\n",
    "2023-10-13_16-23-14 : PReLU 피처 수 50, 최솟값 나오게 추가 학습 진행\n",
    "\n",
    "\n",
    "2023-10-13_16-26-28 : ReLU 피처 수 50, 최솟값 나오게 추가 학습 진행\n",
    "\n",
    "\n",
    "2023-10-13_17-01-48 : PReLU 피처 수 30, kaggle 첫 제출\n",
    "\n",
    "\n",
    "2023-10-13_17-17-28 : LeakyReLU 피처 수 30, kaggle 2 제출\n",
    "\n",
    "\n",
    "2023-10-13_17-27-01 : PReLU 피처 수 30, 정지 조건 0.001\n",
    "\n",
    "\n",
    "2023-10-13_17-29-47 : PReLU 피처 수 30, 정지 조건 0.005\n",
    "\n",
    "\n",
    "2023-10-13_17-31-15 : PReLU 피처 수 30, 정지 조건 0.003 전부 죽음 kaggle 3 제출\n",
    "\n",
    "\n",
    "2023-10-13_17-34-58 : PReLU 피처 수 30, 정지 조건 0.002\n",
    "\n",
    "\n",
    "2023-10-13_17-37-33 : LeakyReLU 피처 수 30, 정지 조건 0.003\n",
    "\n",
    "\n",
    "2023-10-13_17-39-23 : LeakyReLU 피처 수 30, 정지 조건 0.001 전부 다 살음\n",
    "\n",
    "\n",
    "2023-10-13_17-41-06 : LeakyReLU 피처 수 30, 정지 조건 0.002 전부 다 살음\n",
    "\n",
    "\n",
    "2023-10-13_17-45-01 : LeakyReLU 피처 수 30, 정지 조건 최초 에포크 내부에서 실행\n",
    "\n",
    "\n",
    "2023-10-13_17-46-05 : PReLU 피처 수 30, 내부 정지 조건 0.001 정지 안함, 전부 살음\n",
    "\n",
    "\n",
    "2023-10-13_17-49-52 : PReLU 피처 수 30, 내부 정지 조건 0.003 빨리 멈춤, 전부 살음\n",
    "\n",
    "\n",
    "2023-10-13_17-52-35 : PReLU 피처 수 30 정지 안시킴 kaggle 4 제출\n",
    "\n",
    "\n",
    "2023-10-14_20-23-34 : PReLU 레이어 수 증가, 피처 수 30 정지 안시킴\n",
    "\n",
    "\n",
    "2023-10-14_20-32-51 : PReLU 레이어 수 증가, 피처 수 30 정지 200 kaggle 5 제출\n",
    "\n",
    "\n",
    "2023-10-14_20-36-15 : PReLU 레이어 수 증가(레이블 내 피처 수 줄어들게 변경), 정지 안함 kaggle 6 제출\n",
    "\n",
    "\n",
    "2023-10-14_20-38-41 : PReLU 레이어 수 증가(레이블 내 피처 수 줄어들게 변경), 정지 100 kaggle 7 제출\n",
    "\n",
    "\n",
    "__위 과정으로 에포크의 길이가 불필요하게 많아 질 경우 loss값은 줄더라도 test에서 이상한 값을 출력하게 됨(전부 살거나 죽거나), 에포크는 100정도로도 충분하다고 판단됨__\n",
    "\n",
    "\n",
    "__레이어의 경우에도 2개는 너무 적다고 판단되어 레이어 수 추가 및 레이어 내 피처 수를 내림차순으로 구성되게 코드 변경__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b246ef-2790-4158-ac0a-250b39e6584a",
   "metadata": {},
   "source": [
    "### wandb 사이트 (요구 사항 2와 동일)\n",
    "https://wandb.ai/pkqwaszx/my_model_training?workspace=user-pkqwaszx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3a95e-2ebe-4f5d-bfb7-29c924394be6",
   "metadata": {},
   "source": [
    "## 요구 사항 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c451bab-5901-4a7a-b8e3-8ee72ba477db",
   "metadata": {},
   "source": [
    "![kaggle_rank](https://github.com/ratatuwee/d_l_h_w/blob/main/%ED%99%94%EB%A9%B4%20%EC%BA%A1%EC%B2%98%202023-10-14%20210751.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efa850-0b0c-4557-a2d3-6144b70eb403",
   "metadata": {},
   "source": [
    "## 요구사항 5\n",
    "wandb 사이트\n",
    "\n",
    "\n",
    "https://wandb.ai/pkqwaszx/my_model_training?workspace=user-pkqwaszx\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba10ea5-b7c6-41ce-9f81-9a30089aead1",
   "metadata": {},
   "source": [
    "## 고찰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729d68d-026c-4c90-a8e8-1691bea6ddf9",
   "metadata": {},
   "source": [
    "이번 과제를 진행하면서 딥러닝을 실제로 분석해보고 더 나은 성능을 보이기 위하여 여러가지 시도를 해보며 코드들을 자세히 이해할 수 있는 시간이였다. 아직은 실력이 부족하여 kaggle에서의 아주 높은 점수를 맞지 못하며 아쉬웠고 딥러닝에 대하여 더 자세히 알고싶다는 생각이 드는 과제였다. kaggle에서 높은 점수를 맞을 수 있을지 승부욕이 생겨 과제에서 요구하는 사항과 추가적으로 딥러닝의 다른 코드들을 살펴보고 코드를 보완해보았지만 어떠한 식으로 해야더 높은 점수를 기록할 수 있을지 아직은 잘 모르겠어 이 부분이 아쉽다고 생각하였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
